{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import h5py\n",
    "import logging\n",
    "import tensorflow as tf\n",
    "from math import log10\n",
    "from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Iterator, Tuple\n",
    "from dataset_generator import DatasetGenerator\n",
    "from layers import weight_variable,bias_variable,conv2d,weight_variable_devonc,deconv2d,max_pool,pixel_wise_softmax,cross_entropy,features_concat,pixel_wise_softmax_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########\n",
    "GLOBAL_PATH='MODEL_FIRST/'\n",
    "##########\n",
    "\n",
    "if not os.path.exists(GLOBAL_PATH):\n",
    "            os.makedirs(GLOBAL_PATH)\n",
    "        \n",
    "#############\n",
    "PATH_TRAINING='TRAINING/'\n",
    "PATH_VALIDATION='VALIDATION/'\n",
    "PATH_TEST='TEST/'\n",
    "\n",
    "PATH_INPUT='INPUT/'\n",
    "PATH_OUTPUT='OUTPUT/'\n",
    "##############\n",
    "\n",
    "        \n",
    "INPUT_CHANNELS=9\n",
    "OUTPUT_CHANNELS=1\n",
    "NB_CLASSES=2\n",
    "\n",
    "SIZE_PATCH=500\n",
    "##############\n",
    "MODEL_PATH_SAVE=GLOBAL_PATH+'SRCNN.ckpt'\n",
    "MODEL_PATH_RESTORE=''\n",
    "TEST_SAVE=GLOBAL_PATH+'TEST_SAVE/'\n",
    "if not os.path.exists(test_save):\n",
    "            os.makedirs(test_save)\n",
    "        \n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(message)s')\n",
    "\n",
    "##############\n",
    "DEFAULT_VALID=4\n",
    "rec_save=1\n",
    "DROPOUT=0.75\n",
    "DEFAULT_BATCH_SIZE = 36\n",
    "DEFAULT_EPOCHS = 2#500\n",
    "DEFAULT_ITERATIONS =3#850\n",
    "DEFAULT_VALID=4\n",
    "DISPLAY_STEP=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_conv_net(x, keep_prob, channels, n_class, layers=3, features_root=16, filter_size=3, pool_size=2, summaries=True):\n",
    "    \"\"\"\n",
    "    Creates a new convolutional unet for the given parametrization.\n",
    "    \n",
    "    :param x: input tensor, shape [?,nx,ny,channels]\n",
    "    :param keep_prob: dropout probability tensor\n",
    "    :param channels: number of channels in the input image\n",
    "    :param n_class: number of output labels\n",
    "    :param layers: number of layers in the net\n",
    "    :param features_root: number of features in the first layer\n",
    "    :param filter_size: size of the convolution filter\n",
    "    :param pool_size: size of the max pooling operation\n",
    "    :param summaries: Flag if summaries should be created\n",
    "    \"\"\"\n",
    "    \n",
    "    logging.info(\"Layers {layers}, features {features}, filter size {filter_size}x{filter_size}, pool size: {pool_size}x{pool_size}\".format(layers=layers,\n",
    "                                                                                                           features=features_root,\n",
    "                                                                                                           filter_size=filter_size,\n",
    "                                                                                                           pool_size=pool_size))\n",
    "    # Placeholder for the input image\n",
    "    nx = tf.shape(x)[1]\n",
    "    ny = tf.shape(x)[2]\n",
    "    x_image = tf.reshape(x, tf.stack([-1,nx,ny,channels]))\n",
    "    in_node = x_image\n",
    "    batch_size = tf.shape(x_image)[0]\n",
    " \n",
    "    weights = []\n",
    "    biases = []\n",
    "    convs = []\n",
    "    pools = OrderedDict()\n",
    "    deconv = OrderedDict()\n",
    "    dw_h_convs = OrderedDict()\n",
    "    up_h_convs = OrderedDict()\n",
    "    \n",
    "    in_size = 1000\n",
    "    size = in_size\n",
    "    # down layers\n",
    "    for layer in range(0, layers):\n",
    "        features = 2**layer*features_root\n",
    "        stddev = np.sqrt(2 / (filter_size**2 * features))\n",
    "        if layer == 0:\n",
    "            w1 = weight_variable([filter_size, filter_size, channels, features], stddev)\n",
    "        else:\n",
    "            w1 = weight_variable([filter_size, filter_size, features//2, features], stddev)\n",
    "            \n",
    "        w2 = weight_variable([filter_size, filter_size, features, features], stddev)\n",
    "        b1 = bias_variable([features])\n",
    "        b2 = bias_variable([features])\n",
    "        \n",
    "\n",
    "        conv1 = conv2d(in_node, w1, keep_prob)\n",
    "        tmp_h_conv = tf.nn.relu(conv1 + b1)\n",
    "        conv2 = conv2d(tmp_h_conv, w2, keep_prob)\n",
    "        dw_h_convs[layer] = tf.nn.relu(conv2 + b2)\n",
    "        \n",
    "        weights.append((w1, w2))\n",
    "        biases.append((b1, b2))\n",
    "        convs.append((conv1, conv2))\n",
    "        \n",
    "        size -= 4\n",
    "        if layer < layers-1:\n",
    "            pools[layer] = max_pool(dw_h_convs[layer], pool_size)\n",
    "            in_node = pools[layer]\n",
    "            size /= 2\n",
    "        \n",
    "    in_node = dw_h_convs[layers-1]\n",
    "        \n",
    "    # up layers\n",
    "    for layer in range(layers-2, -1, -1):\n",
    "        features = 2**(layer+1)*features_root\n",
    "        stddev = np.sqrt(2 / (filter_size**2 * features))\n",
    "        \n",
    "        wd = weight_variable_devonc([pool_size, pool_size, features//2, features], stddev)\n",
    "        bd = bias_variable([features//2])\n",
    "        h_deconv = tf.nn.relu(deconv2d(in_node, wd, pool_size) + bd)\n",
    "        h_deconv_concat = features_concat(dw_h_convs[layer], h_deconv)\n",
    "        deconv[layer] = h_deconv_concat\n",
    "        \n",
    "        w1 = weight_variable([filter_size, filter_size, features, features//2], stddev)\n",
    "        w2 = weight_variable([filter_size, filter_size, features//2, features//2], stddev)\n",
    "        b1 = bias_variable([features//2])\n",
    "        b2 = bias_variable([features//2])\n",
    "        \n",
    "        conv1 = conv2d(h_deconv_concat, w1, keep_prob)\n",
    "        h_conv = tf.nn.relu(conv1 + b1)\n",
    "        conv2 = conv2d(h_conv, w2, keep_prob)\n",
    "        in_node = tf.nn.relu(conv2 + b2)\n",
    "        up_h_convs[layer] = in_node\n",
    "\n",
    "        weights.append((w1, w2))\n",
    "        biases.append((b1, b2))\n",
    "        convs.append((conv1, conv2))\n",
    "        \n",
    "        size *= 2\n",
    "        size -= 4\n",
    "\n",
    "    # Output Map\n",
    "    weight = weight_variable([1, 1, features_root, n_class], stddev)\n",
    "    bias = bias_variable([n_class])\n",
    "    conv = conv2d(in_node, weight, tf.constant(1.0))\n",
    "    output_map = tf.nn.relu(conv + bias)\n",
    "    up_h_convs[\"out\"] = output_map\n",
    "    \n",
    "\n",
    "    variables = []\n",
    "    for w1,w2 in weights:\n",
    "        variables.append(w1)\n",
    "        variables.append(w2)\n",
    "        \n",
    "    for b1,b2 in biases:\n",
    "        variables.append(b1)\n",
    "        variables.append(b2)\n",
    "\n",
    "    \n",
    "    return output_map, variables, int(in_size - size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCNN():\n",
    "    \n",
    "    def __init__(self, channels=INPUT_CHANNELS, n_class=NB_CLASSES, cost_kwargs={}, **kwargs):\n",
    "        \"\"\"\n",
    "        Initializes the custom CNN \n",
    "        \"\"\"\n",
    "        self.n_class = n_class\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        self.X_placeholder = tf.placeholder(tf.float32, [None, None, None, INPUT_CHANNELS], name='X_placeholder')\n",
    "        self.y_placeholder = tf.placeholder(tf.float32, [None, None, None,NB_CLASSES], name='y_placeholder')\n",
    "        self.keep_prob = tf.placeholder(tf.float32)\n",
    "        \n",
    "        logits, self.variables, self.offset = create_conv_net(self.X_placeholder, self.keep_prob, channels, n_class, **kwargs)\n",
    "        \n",
    "        self.cross_entropy = tf.reduce_mean(cross_entropy(tf.reshape(self.y_placeholder, [-1, n_class]),\n",
    "                                                          tf.reshape(pixel_wise_softmax_2(logits), [-1, n_class])))\n",
    "        self.cost=self.cross_entropy\n",
    "        self.gradients_node = tf.gradients(self.cross_entropy, self.variables)\n",
    "        \n",
    "        self.predicter = pixel_wise_softmax_2(logits)\n",
    "        self.correct_pred = tf.equal(tf.argmax(self.predicter, 3), tf.argmax(self.y_placeholder, 3))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(self.correct_pred, tf.float32))\n",
    "        \n",
    "        \n",
    "    def save(self, sess,saver, model_path,step):\n",
    "        \"\"\"\n",
    "        Saves the current session to a checkpoint\n",
    "        \n",
    "        :param sess: current session\n",
    "        :param saver: saver of the model\n",
    "        :param model_path: path to file system location\n",
    "        :step: step of the iterations during training when the model is stored\n",
    "        \"\"\"\n",
    "       \n",
    "        save_path=saver.save(sess, model_path,global_step=step)\n",
    "        \n",
    "        return save_path\n",
    "\n",
    "    \n",
    "    def restore(self, sess,saver, model_path):\n",
    "        \"\"\"\n",
    "        Restores a session from a checkpoint\n",
    "        \n",
    "        :param sess: current session instance\n",
    "        :param saver: saver of the model\n",
    "        :param model_path: path to file system checkpoint location\n",
    "        \"\"\"\n",
    "\n",
    "        print(\"Reading checkpoints...\")\n",
    "        saver.restore(sess, model_path)\n",
    "        summary=\"Model restored from file: %s\" % (model_path)\n",
    "        print(summary)\n",
    "        return True\n",
    "        \n",
    "    def predict(self, model_path, x_test):\n",
    "        \"\"\"\n",
    "        Uses the model to create a prediction for the given data\n",
    "        \n",
    "        :param model_path: path to the model checkpoint to restore\n",
    "        :param x_test: Data to predict on. Shape [n, nx, ny, channels]\n",
    "        :returns prediction: The unet prediction Shape [n, px, py, labels] (px=nx-self.offset/2) \n",
    "        \"\"\"\n",
    "        \n",
    "        init = tf.global_variables_initializer()\n",
    "        with tf.Session() as sess:\n",
    "            # Initialize variables\n",
    "            sess.run(init)\n",
    "        \n",
    "            # Restore model weights from previously saved model\n",
    "            self.restore(sess, model_path)\n",
    "            \n",
    "            y_dummy = np.empty((x_test.shape[0], x_test.shape[1], x_test.shape[2], self.n_class))\n",
    "            prediction = sess.run(self.predicter, feed_dict={self.X_placeholder: x_test, self.y_placeholder: y_dummy, self.keep_prob: 1.})\n",
    "            \n",
    "        return prediction\n",
    "        \n",
    "#     def predict_tempo(self, X_test,Y_test):\n",
    " \n",
    "#         init = tf.global_variables_initializer()\n",
    "#         with tf.Session() as sess:\n",
    "#             # Initialize variables\n",
    "#             sess.run(init)\n",
    "        \n",
    "# #             # Restore model weights from previously saved model\n",
    "# #             self.restore(sess, model_path)\n",
    "            \n",
    "#             y_dummy = np.empty((X_test.shape[0], X_test.shape[1], X_test.shape[2], self.n_class))\n",
    "#             prediction = sess.run(self.predicter, feed_dict={self.X_placeholder: X_test, self.y_placeholder: Y_test, self.keep_prob: 1.})\n",
    "            \n",
    "#         return prediction\n",
    "        \n",
    "        \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    " class Trainer(object):\n",
    "    \"\"\"\n",
    "    Trains a unet instance\n",
    "    \n",
    "    :param net: the unet instance to train\n",
    "    :param batch_size: size of training batch\n",
    "    :param norm_grads: (optional) true if normalized gradients should be added to the summaries\n",
    "    :param optimizer: (optional) name of the optimizer to use (momentum or adam)\n",
    "    :param opt_kwargs: (optional) kwargs passed to the learning rate (momentum opt) and to the optimizer ->learning_rate, decay_rate,momentum (momentum) or learning_rate (adam)   \n",
    "    \"\"\"\n",
    "    def __init__(self, net, batch_size=DEFAULT_BATCH_SIZE, optimizer=\"momentum\", opt_kwargs={}):\n",
    "        self.net = net\n",
    "        self.batch_size = batch_size\n",
    "        self.optimizer = optimizer\n",
    "        self.opt_kwargs = opt_kwargs\n",
    "        \n",
    "    def _get_optimizer(self, training_iters, global_step):\n",
    "        if self.optimizer == \"momentum\":\n",
    "            learning_rate = self.opt_kwargs.pop(\"learning_rate\", 0.2)\n",
    "            decay_rate = self.opt_kwargs.pop(\"decay_rate\", 0.95)\n",
    "            momentum = self.opt_kwargs.pop(\"momentum\", 0.2)\n",
    "            \n",
    "            self.learning_rate_node = tf.train.exponential_decay(learning_rate=learning_rate, \n",
    "                                                        global_step=global_step, \n",
    "                                                        decay_steps=training_iters,  \n",
    "                                                        decay_rate=decay_rate, \n",
    "                                                        staircase=True)\n",
    "            \n",
    "            optimizer = tf.train.MomentumOptimizer(learning_rate=self.learning_rate_node, momentum=momentum,\n",
    "                                                   **self.opt_kwargs).minimize(self.net.cost, \n",
    "                                                                                global_step=global_step)\n",
    "        elif self.optimizer == \"adam\":\n",
    "            learning_rate = self.opt_kwargs.pop(\"learning_rate\", 0.001)\n",
    "            self.learning_rate_node = tf.Variable(learning_rate)\n",
    "            \n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate_node, \n",
    "                                               **self.opt_kwargs).minimize(self.net.cost,\n",
    "                                                                     global_step=global_step)\n",
    "        \n",
    "        return optimizer\n",
    "    \n",
    "    def _initialize(self, training_iters, output_path, restore, prediction_path):\n",
    "        global_step = tf.Variable(0)\n",
    "        init = tf.global_variables_initializer()\n",
    "        self.norm_gradients_node = tf.Variable(tf.constant(0.0, shape=[len(self.net.gradients_node)]))\n",
    "\n",
    "        self.optimizer = self._get_optimizer(training_iters, global_step)        \n",
    "        \n",
    "        \n",
    "        self.prediction_path = prediction_path\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        return init\n",
    "    \n",
    "    def train(self, data_provider_path, save_path=MODEL_PATH_SAVE, restore_path=MODEL_PATH_RESTORE, training_iters=DEFAULT_ITERATIONS, epochs=DEFAULT_EPOCHS, dropout=DROPOUT, display_step=DISPLAY_STEP, validation_batch_size=DEFAULT_VALID,restore=False, prediction_path = TEST_SAVE):\n",
    "        \"\"\"\n",
    "        Lauches the training process\n",
    "        \n",
    "        :param data_provider_path: where the DATASET folder is\n",
    "        :param save_path: path where to store checkpoints\n",
    "        :param restore_path: path where is the model to restore is stored\n",
    "        :param training_iters: number of training mini batch iteration\n",
    "        :param epochs: number of epochs\n",
    "        :param dropout: dropout probability\n",
    "        :param display_step: number of steps till outputting stats\n",
    "        :param restore: Flag if previous model should be restored \n",
    "        :param prediction_path: path where to save predictions on each epoch\n",
    "        \"\"\"\n",
    "        \n",
    "        PATH_TRAINING=data_provider_path+'TRAINING/'\n",
    "        PATH_VALIDATION=data_provider_path+'VALIDATION/'\n",
    "        PATH_TEST=data_provider_path+'TEST/'\n",
    "        \n",
    "        if epochs == 0:\n",
    "            return save_path\n",
    "        \n",
    "        init = self._initialize(training_iters, save_path, restore, prediction_path)\n",
    "        saver = tf.train.Saver()\n",
    "        with tf.Session() as sess:        \n",
    "            sess.run(init)\n",
    "            \n",
    "            if restore:\n",
    "                self.restore(sess,saver,restore_path )\n",
    "            else:\n",
    "                sess.run(init)\n",
    "\n",
    "            \n",
    "\n",
    "            val_generator = DatasetGenerator.from_root_folder(PATH_VALIDATION, batch_size=validation_batch_size)\n",
    "            val_generator=val_generator.shuffled()\n",
    "            val_generator=val_generator.__iter__()\n",
    "            X_val,Y_val=val_generator.__next__()\n",
    "            X_val=standardize(X_val)\n",
    "            \n",
    "            self.store_prediction(sess, X_val, Y_val, \"_init\")\n",
    "            \n",
    "            logging.info(\"Start optimization\")\n",
    "            \n",
    "#             avg_gradients = None\n",
    "#             for epoch in range(epochs):\n",
    "#                 total_loss = 0\n",
    "#                 for step in range((epoch*training_iters), ((epoch+1)*training_iters)):\n",
    "#                     batch_x, batch_y = data_provider(self.batch_size)\n",
    "                     \n",
    "#                     # Run optimization op (backprop)\n",
    "#                     _, loss, lr, gradients = sess.run((self.optimizer, self.net.cost, self.learning_rate_node, self.net.gradients_node), \n",
    "#                                                       feed_dict={self.net.x: batch_x,\n",
    "#                                                                  self.net.y: util.crop_to_shape(batch_y, pred_shape),\n",
    "#                                                                  self.net.keep_prob: dropout})\n",
    "\n",
    "#                     if self.net.summaries and self.norm_grads:\n",
    "#                         avg_gradients = _update_avg_gradients(avg_gradients, gradients, step)\n",
    "#                         norm_gradients = [np.linalg.norm(gradient) for gradient in avg_gradients]\n",
    "#                         self.norm_gradients_node.assign(norm_gradients).eval()\n",
    "                    \n",
    "#                     if step % display_step == 0:\n",
    "#                         self.output_minibatch_stats(sess, summary_writer, step, batch_x, util.crop_to_shape(batch_y, pred_shape))\n",
    "                        \n",
    "#                     total_loss += loss\n",
    "\n",
    "#                 self.output_epoch_stats(epoch, total_loss, training_iters, lr)\n",
    "#                 self.store_prediction(sess, test_x, test_y, \"epoch_%s\"%epoch)\n",
    "                    \n",
    "#                 save_path = self.net.save(sess, save_path)\n",
    "#             logging.info(\"Optimization Finished!\")\n",
    "            \n",
    "            return save_path\n",
    "    \n",
    "    def store_prediction(self, sess, batch_x, batch_y, name):\n",
    "        prediction = sess.run(self.net.predicter, feed_dict={self.net.X_placeholder: batch_x, \n",
    "                                                             self.net.y_placeholder: batch_y, \n",
    "                                                             self.net.keep_prob: 1.})\n",
    "        \n",
    "        loss = sess.run(self.net.cost, feed_dict={self.net.X_placeholder: batch_x, \n",
    "                                                       self.net.y_placeholder: batch_y, \n",
    "                                                       self.net.keep_prob: 1.})\n",
    "\n",
    "        logging.info(\"Verification error= {:.1f}%, loss= {:.4f}\".format(error_rate(prediction,batch_y),loss))\n",
    "              \n",
    "#         img = util.combine_img_prediction(batch_x, batch_y, prediction)\n",
    "#         util.save_image(img, \"%s/%s.jpg\"%(self.prediction_path, name))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(data):\n",
    "    '''\n",
    "    Standardize the input data of the network\n",
    "    :param data to be standardized (size nb_batches x WIDTH x HEIGHT x number of channels) \n",
    "    \n",
    "    returns data standardized size nb_batches x WIDTH x HEIGHT x number of channels \n",
    "    \n",
    "    '''\n",
    "    WIDTH=data.shape[1]\n",
    "    HEIGHT=data.shape[2]\n",
    "    channels=data.shape[3]\n",
    "    \n",
    "    mean_t=np.mean(np.reshape(data,[len(data)*WIDTH*HEIGHT,channels]), axis=0)\n",
    "    std_t=np.std(np.reshape(data,[len(data)*WIDTH*HEIGHT,channels]), axis=0)\n",
    "    data=data-mean_t/std_t\n",
    "    \n",
    "    #For normalization \n",
    "    min_t=np.amin(np.reshape(data,[len(data)*WIDTH*HEIGHT,channels]), axis=0)\n",
    "    max_t=np.amax(np.reshape(data,[len(data)*WIDTH*HEIGHT,channels]), axis=0)\n",
    "    data=(data-min_t)/(max_t-min_t)\n",
    "\n",
    "    return data\n",
    "\n",
    "def error_rate(predictions, labels):\n",
    "        \"\"\"\n",
    "        Return the error rate based on dense predictions and 1-hot labels.\n",
    "        \"\"\"\n",
    "\n",
    "        return 100.0 - (\n",
    "            100.0 *\n",
    "            np.sum(np.argmax(predictions, 3) == np.argmax(labels, 3)) /\n",
    "            (predictions.shape[0]*predictions.shape[1]*predictions.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-06 14:33:16,440 Layers 3, features 16, filter size 3x3, pool size: 2x2\n",
      "2018-03-06 14:33:19,144 Verification error= 98.3%, loss= 0.3608\n",
      "2018-03-06 14:33:19,145 Start optimization\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    model = CustomCNN()\n",
    "    \n",
    "    \n",
    "    root_folder = '../DATA_GHANA/DATASET/500_x_500_8_bands/'\n",
    "    \n",
    "    trainer=Trainer(model)\n",
    "    trainer.train(root_folder)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    '''\n",
    "    for iteration in range(1):\n",
    "        print('Mini Batch %d'%iteration)\n",
    "        X,Y=generator.__next__()\n",
    "        prediction=model.predict_tempo(X,Y)\n",
    "        for j in range(len(X)):\n",
    "            print('Batch %d'%j)\n",
    "            print('Prediction')\n",
    "            plt.imshow(prediction[j,:,:,0])\n",
    "            plt.show()\n",
    "            print('Groundtruth')\n",
    "            plt.imshow(Y[j,:,:,0])\n",
    "            plt.show()\n",
    "        exit()\n",
    "    '''\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_dhi",
   "language": "python",
   "name": "env_dhi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
