{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/s161362/.conda/envs/env_dhi/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from numpy import newaxis\n",
    "import cv2\n",
    "import sys\n",
    "import os\n",
    "from image_utils import read_data_h5,write_data_h5\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading groundtruth.h5\n",
      "Reading panchro.h5\n",
      "Reading pansharpened_0.h5\n",
      "Reading pansharpened_1.h5\n",
      "Reading pansharpened_2.h5\n",
      "Reading pansharpened_3.h5\n",
      "Reading pansharpened_4.h5\n",
      "Reading pansharpened_5.h5\n",
      "Reading pansharpened_6.h5\n",
      "Reading pansharpened_7.h5\n",
      "Dataset read\n",
      "Dataset shuffled\n",
      "Split (TRAINING - VALIDATION:0.800000) - TEST:0.800000  done\n",
      "Training size:4958, Validation size:1240, Test size: 1549\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    \n",
    "\n",
    "    path_patches='../DATA_GHANA/RAW_PATCHES/120_x_120/'\n",
    "    path_dataset='../DATA_GHANA/DATASET/120_x_120/'\n",
    "    if not os.path.exists(path_dataset):\n",
    "            os.makedirs(path_dataset)\n",
    "    \n",
    "   \n",
    "    training_ratio=0.8 #so    test_ratio=0.2\n",
    "    validation_ratio=0.2\n",
    "    \n",
    "    \n",
    "    list_input_panchro=[]\n",
    "    list_input_ms=[]\n",
    "    list_input_pansharp=[]\n",
    "    for filename in sorted(os.listdir(path_patches)):\n",
    "        if filename.startswith('panchro'):\n",
    "            print('Reading %s'%filename)\n",
    "            list_input_panchro.append(read_data_h5(path_patches+filename))\n",
    "#         if filename.startswith('ms'):\n",
    "#             print('Reading %s'%filename)\n",
    "#             list_input_ms.append(read_data_h5(path_patches+filename))\n",
    "        if filename.startswith('pansharp'):\n",
    "            print('Reading %s'%filename)\n",
    "            list_input_pansharp.append(read_data_h5(path_patches+filename))\n",
    "        if filename.startswith('groundtruth'):\n",
    "            print('Reading %s'%filename)\n",
    "            list_output=read_data_h5(path_patches+filename)\n",
    "            \n",
    "#     ##BUILD Panchro + Pansharp --> 9 bands\n",
    "    \n",
    "    list_input_panchro=np.squeeze(np.asarray(list_input_panchro))[newaxis,:,:,:]\n",
    "    list_input_pansharp=np.squeeze(np.asarray(list_input_pansharp))\n",
    "    list_input=np.concatenate((list_input_panchro,list_input_pansharp),axis=0)\n",
    "    \n",
    "#     ## Build Pancrho + MS --> 9 bands\n",
    "    \n",
    "#     list_input_panchro=np.squeeze(np.asarray(list_input_panchro))[newaxis,:,:,:]\n",
    "#     list_input_ms=np.squeeze(np.asarray(list_input_ms))\n",
    "#     list_input=np.concatenate((list_input_panchro,list_input_ms),axis=0)\n",
    "    \n",
    "#     ## Build Panchro + Pansharp 2,3,5,7\n",
    "\n",
    "#     list_input_panchro=np.squeeze(np.asarray(list_input_panchro))[newaxis,:,:,:]\n",
    "#     list_input_pansharp=np.stack((np.squeeze(np.asarray(list_input_pansharp))[1,:,:,:],\n",
    "#                                         np.squeeze(np.asarray(list_input_pansharp))[2,:,:,:],\n",
    "#                                         np.squeeze(np.asarray(list_input_pansharp))[4,:,:,:],\n",
    "#                                         np.squeeze(np.asarray(list_input_pansharp))[6,:,:,:]),axis=0)\n",
    "#     list_input=np.concatenate((list_input_panchro,list_input_pansharp),axis=0)\n",
    "\n",
    "    ## Build Panchro + Pansharp 2,3,5,7 + MS -->14 bands\n",
    "    \n",
    "#     list_input_panchro=np.squeeze(np.asarray(list_input_panchro))[newaxis,:,:,:]\n",
    "#     list_input_pansharp=np.stack((np.squeeze(np.asarray(list_input_pansharp))[1,:,:,:],\n",
    "#                                         np.squeeze(np.asarray(list_input_pansharp))[2,:,:,:],\n",
    "#                                         np.squeeze(np.asarray(list_input_pansharp))[4,:,:,:],\n",
    "#                                         np.squeeze(np.asarray(list_input_pansharp))[6,:,:,:]),axis=0)\n",
    "#     list_input_ms=np.squeeze(np.asarray(list_input_ms))\n",
    "#     list_input=np.concatenate((list_input_panchro,list_input_pansharp,list_input_ms),axis=0)\n",
    "    \n",
    "    \n",
    "    \n",
    "    ## Followup\n",
    "    \n",
    "    list_input=np.transpose(list_input,(1,2,3,0))\n",
    "    \n",
    "    list_output=np.squeeze(list_output)\n",
    "    \n",
    "    print('Dataset read')\n",
    "    idx_shuffle = np.arange(len(list_input))\n",
    "    np.random.shuffle(idx_shuffle)\n",
    "    print('Dataset shuffled')    \n",
    "    list_input=list_input[idx_shuffle]\n",
    "    list_output=list_output[idx_shuffle]\n",
    "    \n",
    "    #Do the split\n",
    "    training_size=int(round(training_ratio*list_input.shape[0]))\n",
    "    test_size=list_input.shape[0]-training_size\n",
    "    validation_size=int(round(validation_ratio*training_size))\n",
    "    training_size=training_size-validation_size\n",
    "    \n",
    "    \n",
    "    print('Split (TRAINING - VALIDATION:%f) - TEST:%f  done'%(1-validation_ratio,training_ratio))\n",
    "    print('Training size:%d, Validation size:%d, Test size: %d'%(training_size,validation_size,test_size))\n",
    "    \n",
    "#     #Save the dataset\n",
    "    \n",
    "#     if not os.path.exists(path_dataset+'TRAINING'):\n",
    "#             os.makedirs(path_dataset+'TRAINING')\n",
    "#             if not os.path.exists(path_dataset+'TRAINING/INPUT'):\n",
    "#                 os.makedirs(path_dataset+'TRAINING/INPUT')\n",
    "#             if not os.path.exists(path_dataset+'TRAINING/OUTPUT'):\n",
    "#                 os.makedirs(path_dataset+'TRAINING/OUTPUT')\n",
    "#     if not os.path.exists(path_dataset+'VALIDATION'):\n",
    "#             os.makedirs(path_dataset+'VALIDATION')\n",
    "#             if not os.path.exists(path_dataset+'VALIDATION/INPUT'):\n",
    "#                 os.makedirs(path_dataset+'VALIDATION/INPUT')\n",
    "#             if not os.path.exists(path_dataset+'VALIDATION/OUTPUT'):\n",
    "#                 os.makedirs(path_dataset+'VALIDATION/OUTPUT')\n",
    "#     if not os.path.exists(path_dataset+'TEST'):\n",
    "#             os.makedirs(path_dataset+'TEST')\n",
    "#             if not os.path.exists(path_dataset+'TEST/INPUT'):\n",
    "#                 os.makedirs(path_dataset+'TEST/INPUT')\n",
    "#             if not os.path.exists(path_dataset+'TEST/OUTPUT'):\n",
    "#                 os.makedirs(path_dataset+'TEST/OUTPUT')\n",
    "            \n",
    "#     print('BUILD TRAINING SET')\n",
    "#     for i in range(training_size):\n",
    "#         print('Patch %d'%i)\n",
    "#         write_data_h5(path_dataset+'TRAINING/INPUT/input_'+str(i)+'.h5',list_input[i,:,:,:])\n",
    "#         write_data_h5(path_dataset+'TRAINING/OUTPUT/output_'+str(i)+'.h5',list_output[i,:,:])\n",
    "        \n",
    "#     print('BUILD VALIDATION SET')\n",
    "#     for i in range(training_size,training_size+validation_size):\n",
    "#         print('Patch %d'%i)\n",
    "#         write_data_h5(path_dataset+'VALIDATION/INPUT/input_'+str(i)+'.h5',list_input[i,:,:,:])\n",
    "#         write_data_h5(path_dataset+'VALIDATION/OUTPUT/output_'+str(i)+'.h5',list_output[i,:,:])\n",
    "        \n",
    "#     print('BUILD TEST SET')\n",
    "#     for i in range(training_size+validation_size,list_input.shape[0]):\n",
    "#         print('Patch %d'%i)\n",
    "#         write_data_h5(path_dataset+'TEST/INPUT/input_'+str(i)+'.h5',list_input[i,:,:,:])\n",
    "#         write_data_h5(path_dataset+'TEST/OUTPUT/output_'+str(i)+'.h5',list_output[i,:,:])\n",
    "            \n",
    "    \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_dhi",
   "language": "python",
   "name": "env_dhi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
