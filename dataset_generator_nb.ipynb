{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/s161362/.conda/envs/env_dhi/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from imgaug import augmenters as iaa\n",
    "import numpy.ma as ma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_INPUT='INPUT/'\n",
    "PATH_OUTPUT='OUTPUT/'\n",
    "SIZE_PATCH=500\n",
    "\n",
    "INPUT_SIZE=9\n",
    "OUTPUT_SIZE=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_images(paths_input,paths_output):\n",
    "    '''\n",
    "    Reads and saves as as an array image input and output\n",
    "    :paths_input array of paths of the input images that have to be read  \n",
    "    :paths_output array of paths of the output images that have to be read  \n",
    "    returns input and output images as array\n",
    "    '''\n",
    "    input_ = []\n",
    "    output_ = []\n",
    "\n",
    "    for path_i in paths_input:\n",
    "        with h5py.File(path_i, 'r') as hf:\n",
    "            X =np.array(hf.get('data'))\n",
    "            input_.append(X)\n",
    "\n",
    "    for path_o in paths_output:\n",
    "        with h5py.File(path_o, 'r') as hf:\n",
    "            Y_build=np.array(hf.get('data'))\n",
    "            Y_build.astype(int)\n",
    "            Y_other= (1-Y_build).astype(int)\n",
    "            Y=np.stack((Y_build,Y_other),axis=2)\n",
    "            output_.append(Y)\n",
    "\n",
    "        \n",
    "            \n",
    "    return np.asarray(input_),np.asarray(output_)\n",
    "        \n",
    "def data_augment(input_,output_,batch_size):\n",
    "    '''\n",
    "    Augments the data with transformation but only select batch_size number of data\n",
    "    :input_ input data\n",
    "    :output_ output data\n",
    "    :batch_size size of the batch\n",
    "    returns input and output arrays of data augmented with transformation or with identity  \n",
    "    '''\n",
    "    seq = iaa.Sequential([iaa.Add((-40, 40)),iaa.AdditiveGaussianNoise(scale=(0, 0.05*255)),iaa.Multiply((0.5, 1.5))])\n",
    "    input_int=input_*255\n",
    "    input_int.astype(int)\n",
    "    pan_new=seq.augment_images(input_int[:,:,:,0])\n",
    "    \n",
    "    pan_new=np.reshape(pan_new,[pan_new.shape[0],pan_new.shape[1],pan_new.shape[2],1])  \n",
    "    input_new=np.concatenate((pan_new,input_int[:,:,:,1:]),axis=-1)\n",
    "    \n",
    "    min_t=np.amin(np.reshape(input_new,[len(input_new)*SIZE_PATCH*SIZE_PATCH,(PAN_SIZE+PXS_SIZE)]), axis=0)\n",
    "    max_t=np.amax(np.reshape(input_new,[len(input_new)*SIZE_PATCH*SIZE_PATCH,(PAN_SIZE+PXS_SIZE)]), axis=0)\n",
    "    input_new=(input_new-min_t)/(max_t-min_t)\n",
    "    \n",
    "    input_tot=np.concatenate((input_,input_new),axis=0)\n",
    "    output_tot=np.concatenate((output_,output_),axis=0)\n",
    "\n",
    "   \n",
    "    idx = np.arange(len(input_tot))\n",
    "    np.random.shuffle(idx)\n",
    "    input_tot=input_tot[idx]\n",
    "    output_tot=output_tot[idx]\n",
    "    \n",
    "\n",
    "    return input_tot[:batch_size,:,:,:],output_tot[:batch_size,:,:,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetGenerator():\n",
    "    '''\n",
    "    DatasetGenerator class\n",
    "    '''\n",
    "\n",
    "    # This decides whether \"unique\" keys should be included in the generator for each datapoint (typically useful for feature caching)\n",
    "    include_keys = False\n",
    "\n",
    "    #img_size = DATA_PATCH_INPUT_SIZE\n",
    "\n",
    "\n",
    "\n",
    "    def __init__(self, paths_input: np.ndarray,paths_output: np.ndarray, batch_size: int = None):\n",
    "        self.paths_input = paths_input\n",
    "        self.paths_output = paths_output\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    @classmethod\n",
    "    def from_root_folder(cls, root_folder: str, *, batch_size: int = None,max_data_size:  int = None):\n",
    "        paths_input = []\n",
    "        paths_output=[]\n",
    "        \n",
    "        \n",
    "        for filename in sorted(os.listdir(root_folder+PATH_INPUT))[:max_data_size]:\n",
    "            paths_input.append(os.path.join(root_folder+PATH_INPUT, filename))\n",
    "\n",
    "        for filename in sorted(os.listdir(root_folder+PATH_OUTPUT))[:max_data_size]:\n",
    "\n",
    "            paths_output.append(os.path.join(root_folder+PATH_OUTPUT, filename))\n",
    "        \n",
    "        \n",
    "        return DatasetGenerator(np.asarray(paths_input), np.asarray(paths_output), batch_size=batch_size)\n",
    "\n",
    "    def shuffled(self, seed=None):\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "\n",
    "        idx = np.arange(len(self.paths_input))\n",
    "        np.random.shuffle(idx)\n",
    "        generator = DatasetGenerator(self.paths_input[idx], self.paths_output[idx],batch_size=self.batch_size)\n",
    "        generator.include_keys = self.include_keys\n",
    "\n",
    "\n",
    "        return generator\n",
    "\n",
    "    def __iter__(self):\n",
    "        if self.batch_size is None:\n",
    "            raise ValueError('Must set a batch size before iterating!')\n",
    "\n",
    "        self.index = 0\n",
    "\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "\n",
    "        while(self.index * self.batch_size) < len(self.paths_input):\n",
    "            start = self.index * self.batch_size\n",
    "            stop = min(start + self.batch_size, len(self.paths_input))\n",
    "\n",
    "            X,Y = _parse_images(self.paths_input[start:stop],self.paths_output[start:stop])\n",
    "\n",
    "\n",
    "            self.index += 1\n",
    "            if self.include_keys:\n",
    "                return self.paths_input[start:stop], X,self.paths_output[start:stop], Y\n",
    "            else:\n",
    "                return X, Y\n",
    "\n",
    "\n",
    "        raise StopIteration\n",
    "    def __data_aug__(self,X,Y):\n",
    "\n",
    "        X,Y=data_augment(X,Y,self.batch_size)\n",
    "\n",
    "        return X,Y\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths_input)\n",
    "\n",
    "    def __getitem__(self, val):\n",
    "        if type(val) is not slice:\n",
    "            raise ValueError('DatasetGenerators can only be sliced')\n",
    "\n",
    "        sliced = DatasetGenerator(self.paths_input[val], self.paths_output[val],batch_size=self.batch_size)\n",
    "        sliced.include_keys = self.include_keys\n",
    "\n",
    "\n",
    "        return sliced\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    root_folder = '../DATA_GHANA/DATASET/500_x_500_8_bands/TRAINING/'\n",
    "    test_save= '../DATA_GHANA/DATASET/500_x_500_8_bands/TEST_SAVE/'\n",
    "    if not os.path.exists(test_save):\n",
    "            os.makedirs(test_save)\n",
    "\n",
    "    batch_size = 5\n",
    "    \n",
    "    generator = DatasetGenerator.from_root_folder(root_folder, batch_size=batch_size)\n",
    "\n",
    "    generator.shuffled()\n",
    "    generator =generator.__iter__()\n",
    "    \n",
    "    \n",
    "    for iteration in range(2):\n",
    "        X,Y=generator.__next__()\n",
    "\n",
    "\n",
    "        for i in range(len(X)):\n",
    "            for j in range(INPUT_SIZE):\n",
    "                plt.imsave(test_save+'X_iter'+str(iteration)+'batch_'+str(i)+'_band_'+str(j)+'.jpg',X[i,:,:,j])\n",
    "            for j in range(OUTPUT_SIZE):\n",
    "                plt.imsave(test_save+'Y_iter'+str(iteration)+'batch_'+str(i)+'_band_'+str(j)+'.jpg',Y[i,:,:,j])\n",
    "        exit()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_dhi",
   "language": "python",
   "name": "env_dhi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
